<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>256151</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell code" data-execution_count="12" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:37}" id="CzleXm-IGyZc" data-outputId="f2acdec4-3a50-4d8d-ddfb-475820afc121">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing tensorflow and checking if google colab gpu is working</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>tf.test.gpu_device_name()</span></code></pre></div>
<div class="output execute_result" data-execution_count="12">
<div class="sourceCode" id="cb2"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span><span class="st">&quot;string&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell code" data-execution_count="13" id="Tj4UjoOZHFtV">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="15" id="_7KqzsY1Rfa1">
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Importing the library to unzip the train and test data files</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>zip_file <span class="op">=</span> <span class="st">&#39;/content/test1.zip&#39;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>extract_path <span class="op">=</span> <span class="st">&#39;test&#39;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>shutil.unpack_archive(zip_file, extract_path)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="16" id="zy4YUlqBTU1m">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>zip_file <span class="op">=</span> <span class="st">&#39;/content/train.zip&#39;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>extract_path <span class="op">=</span> <span class="st">&#39;train&#39;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>shutil.unpack_archive(zip_file, extract_path)</span></code></pre></div>
</div>
<div class="cell code" id="SEZxVdaRVOpy">
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Number of training examples to process before updating our models variables</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>IMG_SHAPE  <span class="op">=</span> <span class="dv">150</span>  </span></code></pre></div>
</div>
<div class="cell code" id="TtET3YwaYqhQ">
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#generate batches of image data for training and validation, with pixel values rescaled and a split between training and validation data.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>image_generator <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>, validation_split <span class="op">=</span> <span class="fl">0.2</span>)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="eCrfs64HYtf5" data-outputId="27b51aa7-8f1d-4413-ace6-53cee6e3504f">
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initiate Generators</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#The generators will resize the images to a target size of 150x150, group them into batches of 32 images, and assign binary labels to the images based on their subdirectory names.</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>train_data_gen <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>train_generator <span class="op">=</span> train_data_gen.flow_from_directory(</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;train&#39;</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        target_size<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>),</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        class_mode<span class="op">=</span><span class="st">&#39;binary&#39;</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>test_data_gen <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>test_generator <span class="op">=</span> test_data_gen.flow_from_directory(</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;test&#39;</span>,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        target_size<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>),</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        class_mode<span class="op">=</span><span class="st">&#39;binary&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 37500 images belonging to 2 classes.
Found 12500 images belonging to 1 classes.
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:253}" id="y8s_b61UYwKV" data-outputId="ab0ed7b0-179b-4428-d87b-3aa4d2d74243">
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a batch of training data</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>train_data_gen <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>).flow_from_directory(</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;train&#39;</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>(IMG_SHAPE, IMG_SHAPE),</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&#39;binary&#39;</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the first batch</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(train_data_gen)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the sample training images</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">0</span>, <span class="dv">5</span>):</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    axes[i].imshow(images[i])</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(labels[i])</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    axes[i].axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 37500 images belonging to 2 classes.
</code></pre>
</div>
<div class="output display_data">
<p><img src="6e69f733d96d764f999402133a36ea100406c580.png" /></p>
</div>
</div>
<div class="cell code" id="REG0ap_qZFew">
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Define the Sequential Model</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>)),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(<span class="dv">2</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div>
</div>
<div class="cell code" id="2Lt07PUOZPC5">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="usujODOSZR90" data-outputId="2e4ea214-72eb-45ed-ffa5-a7b6444182fc">
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>model.summary() <span class="co">#Shows Model Summary</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 148, 148, 32)      896       
                                                                 
 max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         
 2D)                                                             
                                                                 
 conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         
 2D)                                                             
                                                                 
 conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 6272)              0         
                                                                 
 dense (Dense)               (None, 512)               3211776   
                                                                 
 dense_1 (Dense)             (None, 2)                 1026      
                                                                 
=================================================================
Total params: 3,453,634
Trainable params: 3,453,634
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="_YTwh0qFZ099" data-outputId="5d898b04-bb3a-497e-98fd-322875ee65c3">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>validation_generator <span class="op">=</span> test_data_gen.flow_from_directory(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    directory<span class="op">=</span><span class="st">&#39;test&#39;</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>(IMG_SHAPE, IMG_SHAPE),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&#39;binary&#39;</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 12500 images belonging to 1 classes.
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="OY_5CGe2ZUj7" data-outputId="06dd3da0-e0bd-4e00-eae1-9ed1bebc839a">
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>train_steps <span class="op">=</span> train_data_gen.n <span class="op">//</span> train_data_gen.batch_size</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>val_steps <span class="op">=</span> np.ceil(validation_generator.samples <span class="op">/</span> BATCH_SIZE)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit_generator(</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    train_generator,</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>train_steps,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>validation_generator,</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>val_steps</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-32-4a1e7fda2e95&gt;:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
  history = model.fit_generator(
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Epoch 1/10
375/375 [==============================] - 104s 239ms/step - loss: 0.6419 - accuracy: 0.6647 - val_loss: 1.0954 - val_accuracy: 0.0000e+00
Epoch 2/10
375/375 [==============================] - 88s 236ms/step - loss: 0.6364 - accuracy: 0.6685 - val_loss: 1.0659 - val_accuracy: 0.0000e+00
Epoch 3/10
375/375 [==============================] - 86s 230ms/step - loss: 0.6439 - accuracy: 0.6563 - val_loss: 1.0397 - val_accuracy: 0.0000e+00
Epoch 4/10
375/375 [==============================] - 128s 341ms/step - loss: 0.6406 - accuracy: 0.6619 - val_loss: 1.1358 - val_accuracy: 0.0000e+00
Epoch 5/10
375/375 [==============================] - 84s 224ms/step - loss: 0.6398 - accuracy: 0.6634 - val_loss: 1.1495 - val_accuracy: 0.0000e+00
Epoch 6/10
375/375 [==============================] - 85s 226ms/step - loss: 0.6367 - accuracy: 0.6676 - val_loss: 1.0766 - val_accuracy: 0.0000e+00
Epoch 7/10
375/375 [==============================] - 86s 228ms/step - loss: 0.6361 - accuracy: 0.6679 - val_loss: 1.1345 - val_accuracy: 0.0000e+00
Epoch 8/10
375/375 [==============================] - 85s 227ms/step - loss: 0.6389 - accuracy: 0.6640 - val_loss: 1.1221 - val_accuracy: 0.0000e+00
Epoch 9/10
375/375 [==============================] - 84s 224ms/step - loss: 0.6400 - accuracy: 0.6622 - val_loss: 1.1302 - val_accuracy: 0.0000e+00
Epoch 10/10
375/375 [==============================] - 83s 223ms/step - loss: 0.6364 - accuracy: 0.6672 - val_loss: 1.1176 - val_accuracy: 0.0000e+00
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:391}" id="3bLNY5UMZXkS" data-outputId="58090bec-1d97-4009-9cf4-5ddaf4531b8c">
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the accuracy and loss graphs</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> history.history[<span class="st">&#39;accuracy&#39;</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>val_acc <span class="op">=</span> history.history[<span class="st">&#39;val_accuracy&#39;</span>]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> history.history[<span class="st">&#39;loss&#39;</span>]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>val_loss <span class="op">=</span> history.history[<span class="st">&#39;val_loss&#39;</span>]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>epochs_range <span class="op">=</span> <span class="bu">range</span>(EPOCHS)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs_range, acc, label<span class="op">=</span><span class="st">&#39;Training Accuracy&#39;</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs_range, val_acc, label<span class="op">=</span><span class="st">&#39;Validation Accuracy&#39;</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">&#39;lower right&#39;</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Training and Validation Accuracy&#39;</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs_range, loss, label<span class="op">=</span><span class="st">&#39;Training Loss&#39;</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs_range, val_loss, label<span class="op">=</span><span class="st">&#39;Validation Loss&#39;</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">&#39;upper right&#39;</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Training and Validation Loss&#39;</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;./foo.png&#39;</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="cca839d36e66cc3e0e1c8cb8668fdcdd4dd35a49.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>From the above graph and the accuracy and loss values we can say that even though the accuracy is 66%, val_accuracy is zero, which means that the model did not classify any images correctly in the validation set. This suggests that the model may be overfitting to the training data and not generalizing well to new data.</p>
<p>The next step is to tune the hyperparameters to get the good val_accuracy for the model and then compare it with Vision Transformer test values</p>
</div>
<div class="cell code" id="h3IkWWNogCi-">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#### Hyperparameter tuning for better accuracy of the above code</span></span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="tO8lluvajVu2" data-outputId="2b2b24ce-1e17-4860-cc15-0c99e8689e84">
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>pip install keras<span class="op">-</span>tuner</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Collecting keras-tuner
  Downloading keras_tuner-1.3.4-py3-none-any.whl (172 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.2/172.2 KB 5.8 MB/s eta 0:00:00
ent already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.27.1)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)
Requirement already satisfied: protobuf&lt;=3.20.3 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (3.20.3)
Requirement already satisfied: tensorflow&gt;=2.0 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.12.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (67.6.1)
Requirement already satisfied: tensorboard&lt;2.13,&gt;=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (2.12.0)
Requirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (1.6.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (0.32.0)
Requirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (2.2.0)
Requirement already satisfied: libclang&gt;=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (16.0.0)
Requirement already satisfied: h5py&gt;=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (3.8.0)
Requirement already satisfied: numpy&lt;1.24,&gt;=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (1.22.4)
Requirement already satisfied: absl-py&gt;=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (1.4.0)
Requirement already satisfied: gast&lt;=0.4.0,&gt;=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (0.4.0)
Requirement already satisfied: keras&lt;2.13,&gt;=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (2.12.0)
Requirement already satisfied: flatbuffers&gt;=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (23.3.3)
Requirement already satisfied: six&gt;=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (1.16.0)
Requirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (1.53.0)
Requirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (0.2.0)
Requirement already satisfied: jax&gt;=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (0.4.7)
Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (3.3.0)
Requirement already satisfied: tensorflow-estimator&lt;2.13,&gt;=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (2.12.0)
Requirement already satisfied: wrapt&lt;1.15,&gt;=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (1.14.1)
Requirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow&gt;=2.0-&gt;keras-tuner) (4.5.0)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;keras-tuner) (2.0.12)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;keras-tuner) (1.26.15)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;keras-tuner) (3.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;keras-tuner) (2022.12.7)
Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (0.40.0)
Requirement already satisfied: ml-dtypes&gt;=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax&gt;=0.3.15-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (0.0.4)
Requirement already satisfied: scipy&gt;=1.7 in /usr/local/lib/python3.9/dist-packages (from jax&gt;=0.3.15-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (1.10.1)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (2.17.0)
Requirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (0.7.0)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (3.4.3)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (1.8.1)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (2.2.3)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (0.4.6)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (0.2.8)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (4.9)
Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (5.3.0)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (1.3.1)
Requirement already satisfied: importlib-metadata&gt;=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (6.1.0)
Requirement already satisfied: MarkupSafe&gt;=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (2.1.2)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (3.15.0)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (0.4.8)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.13,&gt;=2.12-&gt;tensorflow&gt;=2.0-&gt;keras-tuner) (3.2.2)
Installing collected packages: kt-legacy, keras-tuner
Successfully installed keras-tuner-1.3.4 kt-legacy-1.0.4
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Cxw5xLLPgxTq" data-outputId="d6e0be8c-b89e-4dff-b605-ca529f00eb9a">
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kerastuner.tuners <span class="im">import</span> RandomSearch</span></code></pre></div>
<div class="output stream stderr">
<pre><code>&lt;ipython-input-61-eeffcd97681f&gt;:6: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.
  from kerastuner.tuners import RandomSearch
</code></pre>
</div>
</div>
<div class="cell code" id="LCAsc7NHhAwX">
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>)),</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Conv2D(<span class="dv">256</span>, (<span class="dv">3</span>,<span class="dv">3</span>), activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.MaxPooling2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Flatten(),</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>),</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.BatchNormalization(),</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div>
</div>
<div class="cell code" id="up-yxestzmmw">
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce the learning rate to avoid overshooting the optimal weights</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span></code></pre></div>
</div>
<div class="cell code" id="wxxlFI1EztFW">
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use binary cross-entropy loss for binary classification problem</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>model1.<span class="bu">compile</span>(optimizer<span class="op">=</span>opt,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="jTyFjpXjmLGn" data-outputId="54155613-042c-4588-8a7a-e532b58dab65">
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>model1.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;sequential_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_12 (Conv2D)          (None, 148, 148, 32)      896       
                                                                 
 max_pooling2d_12 (MaxPoolin  (None, 74, 74, 32)       0         
 g2D)                                                            
                                                                 
 batch_normalization_10 (Bat  (None, 74, 74, 32)       128       
 chNormalization)                                                
                                                                 
 conv2d_13 (Conv2D)          (None, 72, 72, 64)        18496     
                                                                 
 max_pooling2d_13 (MaxPoolin  (None, 36, 36, 64)       0         
 g2D)                                                            
                                                                 
 batch_normalization_11 (Bat  (None, 36, 36, 64)       256       
 chNormalization)                                                
                                                                 
 conv2d_14 (Conv2D)          (None, 34, 34, 128)       73856     
                                                                 
 max_pooling2d_14 (MaxPoolin  (None, 17, 17, 128)      0         
 g2D)                                                            
                                                                 
 batch_normalization_12 (Bat  (None, 17, 17, 128)      512       
 chNormalization)                                                
                                                                 
 conv2d_15 (Conv2D)          (None, 15, 15, 256)       295168    
                                                                 
 max_pooling2d_15 (MaxPoolin  (None, 7, 7, 256)        0         
 g2D)                                                            
                                                                 
 batch_normalization_13 (Bat  (None, 7, 7, 256)        1024      
 chNormalization)                                                
                                                                 
 flatten_3 (Flatten)         (None, 12544)             0         
                                                                 
 dense_6 (Dense)             (None, 512)               6423040   
                                                                 
 dropout_2 (Dropout)         (None, 512)               0         
                                                                 
 batch_normalization_14 (Bat  (None, 512)              2048      
 chNormalization)                                                
                                                                 
 dense_7 (Dense)             (None, 1)                 513       
                                                                 
=================================================================
Total params: 6,815,937
Trainable params: 6,813,953
Non-trainable params: 1,984
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="xAUL2aqd0BOl" data-outputId="3b8b6ded-b6db-4a79-ecba-67c7d051ed46">
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Increase the number of epochs to allow the model to learn more</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model1.fit(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    train_generator,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>train_generator.n <span class="op">//</span> train_generator.batch_size,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>validation_generator,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>validation_generator.n <span class="op">//</span> validation_generator.batch_size</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/30
1171/1171 [==============================] - 167s 140ms/step - loss: 0.6906 - accuracy: 0.6273 - val_loss: 1.1216 - val_accuracy: 2.4000e-04
Epoch 2/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6479 - accuracy: 0.6589 - val_loss: 1.1162 - val_accuracy: 1.6000e-04
Epoch 3/30
1171/1171 [==============================] - 170s 145ms/step - loss: 0.6472 - accuracy: 0.6619 - val_loss: 1.1195 - val_accuracy: 0.0000e+00
Epoch 4/30
1171/1171 [==============================] - 168s 144ms/step - loss: 0.6452 - accuracy: 0.6636 - val_loss: 1.0107 - val_accuracy: 0.0054
Epoch 5/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6442 - accuracy: 0.6633 - val_loss: 1.1027 - val_accuracy: 0.0000e+00
Epoch 6/30
1171/1171 [==============================] - 170s 145ms/step - loss: 0.6452 - accuracy: 0.6646 - val_loss: 1.0918 - val_accuracy: 0.0000e+00
Epoch 7/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6440 - accuracy: 0.6642 - val_loss: 1.3973 - val_accuracy: 0.0000e+00
Epoch 8/30
1171/1171 [==============================] - 166s 141ms/step - loss: 0.6428 - accuracy: 0.6649 - val_loss: 1.0619 - val_accuracy: 0.0000e+00
Epoch 9/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6435 - accuracy: 0.6654 - val_loss: 1.1183 - val_accuracy: 0.0000e+00
Epoch 10/30
1171/1171 [==============================] - 167s 143ms/step - loss: 0.6427 - accuracy: 0.6653 - val_loss: 1.1483 - val_accuracy: 0.0000e+00
Epoch 11/30
1171/1171 [==============================] - 167s 143ms/step - loss: 0.6424 - accuracy: 0.6658 - val_loss: 1.1727 - val_accuracy: 0.0000e+00
Epoch 12/30
1171/1171 [==============================] - 164s 140ms/step - loss: 0.6415 - accuracy: 0.6660 - val_loss: 1.0884 - val_accuracy: 0.0000e+00
Epoch 13/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6409 - accuracy: 0.6658 - val_loss: 1.0911 - val_accuracy: 2.4000e-04
Epoch 14/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6408 - accuracy: 0.6659 - val_loss: 1.1054 - val_accuracy: 0.0000e+00
Epoch 15/30
1171/1171 [==============================] - 170s 145ms/step - loss: 0.6397 - accuracy: 0.6662 - val_loss: 1.1050 - val_accuracy: 0.0000e+00
Epoch 16/30
1171/1171 [==============================] - 167s 143ms/step - loss: 0.6399 - accuracy: 0.6662 - val_loss: 1.0729 - val_accuracy: 0.0098
Epoch 17/30
1171/1171 [==============================] - 173s 148ms/step - loss: 0.6390 - accuracy: 0.6664 - val_loss: 1.0322 - val_accuracy: 0.0000e+00
Epoch 18/30
1171/1171 [==============================] - 168s 143ms/step - loss: 0.6389 - accuracy: 0.6663 - val_loss: 1.1411 - val_accuracy: 5.6000e-04
Epoch 19/30
1171/1171 [==============================] - 168s 143ms/step - loss: 0.6382 - accuracy: 0.6668 - val_loss: 1.0166 - val_accuracy: 0.0000e+00
Epoch 20/30
1171/1171 [==============================] - 171s 146ms/step - loss: 0.6374 - accuracy: 0.6664 - val_loss: 1.0884 - val_accuracy: 8.0000e-05
Epoch 21/30
1171/1171 [==============================] - 168s 144ms/step - loss: 0.6368 - accuracy: 0.6664 - val_loss: 1.1432 - val_accuracy: 0.0000e+00
Epoch 22/30
1171/1171 [==============================] - 165s 141ms/step - loss: 0.6362 - accuracy: 0.6664 - val_loss: 1.0934 - val_accuracy: 8.0000e-05
Epoch 23/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6349 - accuracy: 0.6666 - val_loss: 1.1184 - val_accuracy: 0.0018
Epoch 24/30
1171/1171 [==============================] - 165s 141ms/step - loss: 0.6344 - accuracy: 0.6662 - val_loss: 1.0586 - val_accuracy: 4.0000e-04
Epoch 25/30
1171/1171 [==============================] - 164s 140ms/step - loss: 0.6318 - accuracy: 0.6663 - val_loss: 1.1244 - val_accuracy: 0.0016
Epoch 26/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6303 - accuracy: 0.6670 - val_loss: 1.0668 - val_accuracy: 0.0059
Epoch 27/30
1171/1171 [==============================] - 169s 144ms/step - loss: 0.6281 - accuracy: 0.6673 - val_loss: 1.0430 - val_accuracy: 0.0167
Epoch 28/30
1171/1171 [==============================] - 168s 143ms/step - loss: 0.6260 - accuracy: 0.6662 - val_loss: 1.0623 - val_accuracy: 0.0236
Epoch 29/30
1171/1171 [==============================] - 171s 146ms/step - loss: 0.6229 - accuracy: 0.6685 - val_loss: 1.0796 - val_accuracy: 0.0364
Epoch 30/30
1171/1171 [==============================] - 166s 142ms/step - loss: 0.6174 - accuracy: 0.6726 - val_loss: 1.0019 - val_accuracy: 0.0674
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>After tuning the hyperparameters by adding more epochs,reducing the learning rate, dropout layer and tweaking the layers we got the accuracy of 67% and val_accuracy of 67% as well, which means that we got a better accuracy as compared to the previous model however, the val_accuracy is low, indicating that the model is not generalizing well to new data. This suggests that the model may still be overfitting to the training data, and more work may be needed to improve its performance.</p>
<p>In the next step, Vision Transfor is implemented. ViT is based on the Transformer architecture originally developed for natural language processing tasks. The Transformer architecture uses multi-head self-attention mechanisms to learn global dependencies between different parts of the input sequence. In ViT, the input image is first divided into a fixed number of non-overlapping patches, which are then treated as a sequence of tokens. The sequence of patch tokens is then passed through a series of Transformer blocks, which encode both spatial and semantic information of the input image.</p>
<p>---- Reason for choosing ViT and CNN comparison ViT has shown promising results on a range of computer vision tasks, including image classification, object detection, and semantic segmentation. It has been shown to be particularly effective on large-scale datasets, where it has achieved state-of-the-art performance while requiring fewer computational resources than traditional CNNs. ViT is also highly modular and can be easily adapted to different input sizes and tasks, making it a versatile architecture for computer vision research.</p>
</div>
<div class="cell code" id="2lX-0f6_XJ_W">
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we evaluate using Vision Transformer (ViT) instead of CNN, and check the accuracy</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="17" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="g0-DGxxx4wrr" data-outputId="22acc6e6-9600-48f5-ef28-36aa1a3c0c94">
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install vit<span class="op">-</span>keras</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install tensorflow<span class="op">-</span>addons</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> vit_keras <span class="im">import</span> vit, utils</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Model</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Dropout, Flatten</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential,Model</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: vit-keras in /usr/local/lib/python3.9/dist-packages (0.1.0)
Requirement already satisfied: validators in /usr/local/lib/python3.9/dist-packages (from vit-keras) (0.20.0)
Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from vit-keras) (1.10.1)
Requirement already satisfied: numpy&lt;1.27.0,&gt;=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy-&gt;vit-keras) (1.22.4)
Requirement already satisfied: decorator&gt;=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators-&gt;vit-keras) (4.4.2)
Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.9/dist-packages (0.19.0)
Requirement already satisfied: typeguard&gt;=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (3.0.2)
Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)
Requirement already satisfied: importlib-metadata&gt;=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard&gt;=2.7-&gt;tensorflow-addons) (6.1.0)
Requirement already satisfied: typing-extensions&gt;=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard&gt;=2.7-&gt;tensorflow-addons) (4.5.0)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata&gt;=3.6-&gt;typeguard&gt;=2.7-&gt;tensorflow-addons) (3.15.0)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="18" id="vM3_hPOe24b8">
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">#initialize a Vision Transformer (ViT) model with the vit_b16 architecture from the vit module.</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>img_size<span class="op">=</span>[<span class="dv">256</span>,<span class="dv">256</span>]</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> vit.vit_b16(</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    image_size<span class="op">=</span>image_size,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    pretrained<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    include_top<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    pretrained_top<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="19" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="C8tCG1OB3CJL" data-outputId="d01448e1-50d6-42e1-b50d-41f4570b2760">
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(base_model.summary())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;vit-b16&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]     0         
                                                                 
 embedding (Conv2D)          (None, 16, 16, 768)       590592    
                                                                 
 reshape_1 (Reshape)         (None, 256, 768)          0         
                                                                 
 class_token (ClassToken)    (None, 257, 768)          768       
                                                                 
 Transformer/posembed_input   (None, 257, 768)         197376    
 (AddPositionEmbs)                                               
                                                                 
 Transformer/encoderblock_0   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_1   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_2   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_3   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_4   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_5   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_6   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_7   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_8   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_9   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_10  ((None, 257, 768),       7087872   
  (TransformerBlock)          (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_11  ((None, 257, 768),       7087872   
  (TransformerBlock)          (None, 12, None, None))            
                                                                 
 Transformer/encoder_norm (L  (None, 257, 768)         1536      
 ayerNormalization)                                              
                                                                 
 ExtractToken (Lambda)       (None, 768)               0         
                                                                 
=================================================================
Total params: 85,844,736
Trainable params: 85,844,736
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="20" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="UC4z_lY_3EDY" data-outputId="35137199-58cb-40e0-b4eb-3fa984438907">
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Freese the model</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers:</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    layer.trainable<span class="op">=</span><span class="va">False</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Base model layers are freezed!!!&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Base model layers are freezed!!!
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="21" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="4VEfnDS93JsB" data-outputId="9639cd19-8629-4455-93e6-7bc84acd9c6e">
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(base_model.summary())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;vit-b16&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]     0         
                                                                 
 embedding (Conv2D)          (None, 16, 16, 768)       590592    
                                                                 
 reshape_1 (Reshape)         (None, 256, 768)          0         
                                                                 
 class_token (ClassToken)    (None, 257, 768)          768       
                                                                 
 Transformer/posembed_input   (None, 257, 768)         197376    
 (AddPositionEmbs)                                               
                                                                 
 Transformer/encoderblock_0   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_1   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_2   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_3   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_4   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_5   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_6   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_7   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_8   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_9   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_10  ((None, 257, 768),       7087872   
  (TransformerBlock)          (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_11  ((None, 257, 768),       7087872   
  (TransformerBlock)          (None, 12, None, None))            
                                                                 
 Transformer/encoder_norm (L  (None, 257, 768)         1536      
 ayerNormalization)                                              
                                                                 
 ExtractToken (Lambda)       (None, 768)               0         
                                                                 
=================================================================
Total params: 85,844,736
Trainable params: 0
Non-trainable params: 85,844,736
_________________________________________________________________
None
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="22" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="_udWZYJN3LxJ" data-outputId="1fe11e08-8a05-4dfe-925f-da67b1ab8ae4">
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding dropout, flatten and dense layer to the frozen model </span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dropout(<span class="fl">0.2</span>)(base_model.output)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Flatten()(x)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">2</span>,<span class="st">&#39;softmax&#39;</span>)(x)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>base_model.<span class="bu">input</span>,outputs<span class="op">=</span>x)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Final Model is:</span><span class="ch">\n</span><span class="st">&quot;</span>,model.summary())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;model_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 256, 256, 3)]     0         
                                                                 
 embedding (Conv2D)          (None, 16, 16, 768)       590592    
                                                                 
 reshape_1 (Reshape)         (None, 256, 768)          0         
                                                                 
 class_token (ClassToken)    (None, 257, 768)          768       
                                                                 
 Transformer/posembed_input   (None, 257, 768)         197376    
 (AddPositionEmbs)                                               
                                                                 
 Transformer/encoderblock_0   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_1   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_2   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_3   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_4   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_5   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_6   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_7   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_8   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_9   ((None, 257, 768),       7087872   
 (TransformerBlock)           (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_10  ((None, 257, 768),       7087872   
  (TransformerBlock)          (None, 12, None, None))            
                                                                 
 Transformer/encoderblock_11  ((None, 257, 768),       7087872   
  (TransformerBlock)          (None, 12, None, None))            
                                                                 
 Transformer/encoder_norm (L  (None, 257, 768)         1536      
 ayerNormalization)                                              
                                                                 
 ExtractToken (Lambda)       (None, 768)               0         
                                                                 
 dropout_1 (Dropout)         (None, 768)               0         
                                                                 
 flatten_1 (Flatten)         (None, 768)               0         
                                                                 
 dense_1 (Dense)             (None, 2)                 1538      
                                                                 
=================================================================
Total params: 85,846,274
Trainable params: 1,538
Non-trainable params: 85,844,736
_________________________________________________________________
Final Model is:
 None
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="23" id="WcLPXYeg3QlN">
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Defining the optimizer and the hyperparameters</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>opt<span class="op">=</span>Adam(lr<span class="op">=</span><span class="fl">0.001</span>, beta_1<span class="op">=</span><span class="fl">0.9</span>, beta_2<span class="op">=</span><span class="fl">0.999</span>, decay<span class="op">=</span><span class="fl">0.0</span>, amsgrad<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="24" id="s_T0hvmd3XR5">
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>,optimizer<span class="op">=</span>opt,metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="25" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="yohYWRRR3c98" data-outputId="d0b84211-2a0e-41d3-e7a7-2a607e9d178b">
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED<span class="op">=</span><span class="dv">123</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>train_datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    width_shift_range<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    height_shift_range<span class="op">=</span><span class="fl">0.4</span>,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    rescale<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span><span class="dv">255</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>test_datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    rescale<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span><span class="dv">255</span>,</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>train_generator <span class="op">=</span> train_datagen.flow_from_directory(</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;train&#39;</span>,</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&#39;rgb&#39;</span>,</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span> img_size,</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&#39;categorical&#39;</span>,</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span>RANDOM_SEED</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>validation_generator <span class="op">=</span> test_datagen.flow_from_directory(</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;test&#39;</span>,</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&#39;rgb&#39;</span>,</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span> img_size,</span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&#39;categorical&#39;</span>,</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>    seed<span class="op">=</span>RANDOM_SEED</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 25000 images belonging to 1 classes.
Found 12500 images belonging to 1 classes.
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="26" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="L28yCvvJ4P6l" data-outputId="2c37c3cf-6997-46e7-ad16-8321e78a08f9">
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>hist <span class="op">=</span> model.fit(</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    train_generator,</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    validation_data <span class="op">=</span> validation_generator</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>end <span class="op">=</span> time.time()</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Total time taken: &quot;</span>,end<span class="op">-</span>start)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/10
391/391 [==============================] - 824s 2s/step - loss: 2.1775 - accuracy: 0.8116 - val_loss: 2.0582 - val_accuracy: 0.9078
Epoch 2/10
391/391 [==============================] - 795s 2s/step - loss: 2.0841 - accuracy: 0.8119 - val_loss: 2.0084 - val_accuracy: 0.9065
Epoch 3/10
391/391 [==============================] - 845s 2s/step - loss: 2.0717 - accuracy: 0.8112 - val_loss: 1.9390 - val_accuracy: 0.8890
Epoch 4/10
391/391 [==============================] - 790s 2s/step - loss: 2.0603 - accuracy: 0.8084 - val_loss: 1.9333 - val_accuracy: 0.8937
Epoch 5/10
391/391 [==============================] - 832s 2s/step - loss: 2.0570 - accuracy: 0.8060 - val_loss: 1.9160 - val_accuracy: 0.8879
Epoch 6/10
391/391 [==============================] - 834s 2s/step - loss: 2.0550 - accuracy: 0.8010 - val_loss: 1.9633 - val_accuracy: 0.9039
Epoch 7/10
391/391 [==============================] - 834s 2s/step - loss: 2.0528 - accuracy: 0.7992 - val_loss: 1.8560 - val_accuracy: 0.8717
Epoch 8/10
391/391 [==============================] - 788s 2s/step - loss: 2.0458 - accuracy: 0.7976 - val_loss: 1.8792 - val_accuracy: 0.8838
Epoch 9/10
391/391 [==============================] - 784s 2s/step - loss: 2.0408 - accuracy: 0.7969 - val_loss: 1.8754 - val_accuracy: 0.8766
Epoch 10/10
391/391 [==============================] - 836s 2s/step - loss: 2.0421 - accuracy: 0.7948 - val_loss: 1.8753 - val_accuracy: 0.8707
Total time taken:  8256.732847213745
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="27" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:641}" id="rBc3kUPA0vdz" data-outputId="ad095d6f-659a-4f37-c329-13bd15b24b02">
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting the figure for accuracy</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>plt.plot(hist.history[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>plt.plot(hist.history[<span class="st">&#39;val_accuracy&#39;</span>])</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Model Accuracy Vision Transformer&#39;</span>)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Accuracy&#39;</span>)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">&#39;train&#39;</span>, <span class="st">&#39;val&#39;</span>], loc<span class="op">=</span><span class="st">&#39;lower right&#39;</span>)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="fd9ae185d1bd112fdbddcd24c87c16752a668620.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="28" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:641}" id="k6IfW5UI2btF" data-outputId="d33b186a-7193-462f-8299-dce3d5fdd9c7">
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting the figure for loss</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>plt.plot(hist.history[<span class="st">&#39;loss&#39;</span>])</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>plt.plot(hist.history[<span class="st">&#39;val_loss&#39;</span>])</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Model Loss Vision Transformer&#39;</span>)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">&#39;train&#39;</span>, <span class="st">&#39;val&#39;</span>], loc<span class="op">=</span><span class="st">&#39;upper right&#39;</span>)</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="32de2373ea3e631d4c38f6ef3f02653b0dfe9bfe.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>From implementing the ViT model and plotting the graphs and values of loss and accuracy, we can say that:</p>
<p>During the training, the loss decreased gradually with each epoch, indicating that the model was learning to fit the training data. The accuracy also increased from epoch 1 to epoch 2 and remained relatively stable after that, with a final accuracy of 79.48% on the training set.</p>
<p>On the validation set, the loss decreased gradually for the first few epochs but then started fluctuating, with a final validation loss of 1.8753. The validation accuracy also fluctuated, with a final accuracy of 87.07%.</p>
<p>Overall, the training and validation metrics suggest that the model was able to learn to classify the images to some extent, but there is still room for improvement.</p>
<hr />
<p>Comparing the CNN and ViT-B16 models</p>
<p>The ViT model seems to have higher accuracy values compared to the CNN model, especially in the validation set. However, it is important to note that the ViT model took significantly longer to train, and the number of epochs used for training was also higher (10 vs 30). Therefore, it is not a direct comparison between the models, and the ViT model may have benefited from the longer training time and more epochs. Additionally, the ViT model is a transformer-based model designed specifically for image classification tasks, while the CNN model used here is a simpler architecture. Overall, the ViT model seems to be a promising approach for image classification tasks, but the choice of architecture and training parameters will depend on the specific problem at hand.</p>
<hr />
<p>Some of the techniques to improve the ViT models can be</p>
<p>Increase the size of the model: The ViT model has various sizes, ranging from ViT-tiny to ViT-huge. By using a larger model, the capacity of the network is increased, allowing it to learn more complex patterns in the data.</p>
<p>Fine-tuning the pre-trained model: Fine-tuning the pre-trained model on the specific task can help the network adapt to the nuances of the data. By unfreezing some of the layers in the pre-trained model and retraining them on the specific task, the model can learn to make better predictions. (We implemented this step for the current model)</p>
<p>Data augmentation: By increasing the amount of data available for training, the network can learn to generalize better to new examples. Techniques such as random cropping, flipping, and rotation can be used to generate new training examples from the existing data.</p>
<p>Learning rate scheduling: By adjusting the learning rate during training, the network can learn more efficiently. Using a higher learning rate at the start of training can help the network converge faster, while gradually reducing the learning rate can help the network fine-tune the learned representations.</p>
<p>Ensemble models: Combining the predictions of multiple models can help to improve the overall performance. By training multiple ViT models with different initialization or architecture, and averaging their predictions, the model's performance can be improved.</p>
<p>Finally we can comment that there is a huge scope to understand, implement and work around the models, and with experiments and experience we will be able to learn.</p>
</div>
</body>
</html>
